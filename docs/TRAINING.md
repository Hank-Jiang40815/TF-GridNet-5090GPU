# è¨“ç·´æµç¨‹èªªæ˜

æœ¬æ–‡æª”èªªæ˜å¦‚ä½•ä½¿ç”¨æœ¬å°ˆæ¡ˆé€²è¡Œ TF-GridNetV2 æ¨¡å‹è¨“ç·´ã€‚

## ğŸ¯ è¨“ç·´æµç¨‹æ¦‚è¦½

```
æº–å‚™è³‡æ–™ â†’ é…ç½®èª¿æ•´ â†’ åŸ·è¡Œè¨“ç·´ â†’ ç›£æ§é€²åº¦ â†’ è¨˜éŒ„çµæœ â†’ è©•ä¼°æ¨¡å‹
```

## ğŸ“‹ è¨“ç·´å‰æª¢æŸ¥æ¸…å–®

- [ ] ç’°å¢ƒå·²æ­£ç¢ºå®‰è£ï¼ˆåŸ·è¡Œé `./scripts/setup_host.sh`ï¼‰
- [ ] Docker image å·²å»ºæ§‹ï¼ˆ`docker-compose build`ï¼‰
- [ ] Smoke test é€šéï¼ˆ`./scripts/run_smoke_test.sh`ï¼‰
- [ ] è³‡æ–™å·²æº–å‚™å¥½ï¼ˆ`/home/sbplab/Hank/ESPnet/TFG-Transfer-Package/data`ï¼‰
- [ ] é…ç½®æª”å·²èª¿æ•´ï¼ˆ`configs/training_rtx5090.yaml`ï¼‰
- [ ] GPU å¯ç”¨ä¸”è¨˜æ†¶é«”å……è¶³ï¼ˆ`nvidia-smi`ï¼‰

## ğŸš€ é–‹å§‹è¨“ç·´

### åŸºæœ¬è¨“ç·´å‘½ä»¤

```bash
./scripts/run_training.sh <experiment_name> [config_file] [skip_memory_test]
```

**åƒæ•¸èªªæ˜**:
- `experiment_name`: å¯¦é©—åç¨±ï¼ˆå¿…å¡«ï¼‰
- `config_file`: é…ç½®æª”è·¯å¾‘ï¼ˆé¸å¡«ï¼Œé è¨­: `/workspace/configs/training_rtx5090.yaml`ï¼‰
- `skip_memory_test`: è·³éè¨˜æ†¶é«”æ¸¬è©¦ï¼ˆé¸å¡«ï¼Œ`true`/`false`ï¼Œé è¨­: `false`ï¼‰

### ç¯„ä¾‹

```bash
# 1. åŸºæº–å¯¦é©—
./scripts/run_training.sh baseline

# 2. ä½¿ç”¨è‡ªè¨‚é…ç½®
./scripts/run_training.sh my-experiment /workspace/configs/my_config.yaml

# 3. å¿«é€Ÿå•Ÿå‹•ï¼ˆè·³éè¨˜æ†¶é«”æ¸¬è©¦ï¼‰
./scripts/run_training.sh quick-test /workspace/configs/training_rtx5090.yaml true
```

## ğŸ“Š ç›£æ§è¨“ç·´

### å¯¦æ™‚æ—¥èªŒ

```bash
# æŸ¥çœ‹æœ€æ–°å¯¦é©—çš„è¨“ç·´æ—¥èªŒ
tail -f experiments/logs/$(ls -t experiments/logs | head -1)/training.log
```

### GPU ç›£æ§

```bash
# å¯¦æ™‚ç›£æ§ GPU
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¨˜æ†¶é«”ä½¿ç”¨
nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv -l 1
```

### TensorBoardï¼ˆå¦‚æœå•Ÿç”¨ï¼‰

```bash
# å•Ÿå‹• TensorBoard æœå‹™
docker-compose --profile tensorboard up tensorboard

# è¨ªå• http://localhost:6006
```

## ğŸ“ å¯¦é©—è¨˜éŒ„

### è‡ªå‹•è¨˜éŒ„

è¨“ç·´è…³æœ¬æœƒè‡ªå‹•ç”¢ç”Ÿå¯¦é©—ç›®éŒ„ï¼š

```
experiments/logs/YYYYMMDD-HHMMSS-<experiment_name>/
â”œâ”€â”€ config.yaml          # é…ç½®å‚™ä»½
â”œâ”€â”€ training.log         # è¨“ç·´æ—¥èªŒ
â”œâ”€â”€ environment.txt      # ç’°å¢ƒè³‡è¨Š
â”œâ”€â”€ git_info.txt         # Git commit
â”œâ”€â”€ experiment.md        # å¯¦é©—è¨˜éŒ„ç¯„æœ¬
â”œâ”€â”€ checkpoints/
â””â”€â”€ results/
```

### æ‰‹å‹•å¡«å¯«å¯¦é©—è¨˜éŒ„

è¨“ç·´é–‹å§‹å¾Œï¼Œç·¨è¼¯ `experiment.md`:

```bash
# æ‰¾åˆ°æœ€æ–°å¯¦é©—ç›®éŒ„
EXP_DIR=$(ls -td experiments/logs/* | head -1)

# ç·¨è¼¯å¯¦é©—è¨˜éŒ„
vim $EXP_DIR/experiment.md
```

**å¿…å¡«é …ç›®**:
- å¯¦é©—ç›®çš„
- é…ç½®è®Šæ›´
- é æœŸçµæœ

**è¨“ç·´å¾Œå¡«å¯«**:
- å¯¦éš›çµæœ
- è§€å¯Ÿèˆ‡åˆ†æ
- ä¸‹ä¸€æ­¥è¨ˆåŠƒ

è©³ç´°æŒ‡å—è«‹åƒé–± [`../experiments/README.md`](../experiments/README.md)ã€‚

## âš™ï¸ é…ç½®èª¿æ•´

### æ‰¹æ¬¡å¤§å°èª¿æ•´

æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´ `configs/training_rtx5090.yaml`:

```yaml
training:
  batch_size: 32  # èª¿æ•´æ­¤å€¼
  
  # å¦‚æœ OOMï¼Œå•Ÿç”¨ gradient accumulation
  gradient_accumulation:
    enabled: true
    steps: 4  # æœ‰æ•ˆæ‰¹æ¬¡ = batch_size * steps
```

**æ¨è–¦å€¼**ï¼ˆRTX 5090 32GBï¼‰:
- ä¿å®ˆ: 32
- å¹³è¡¡: 64
- æ¿€é€²: 128ï¼ˆå¯èƒ½éœ€è¦ gradient checkpointingï¼‰

### å­¸ç¿’ç‡èª¿æ•´

```yaml
training:
  learning_rate: 0.0005  # åŸºæº–å€¼
  
  # æ‰¹æ¬¡å¤§å°ç¿»å€æ™‚ï¼Œå­¸ç¿’ç‡ä¹Ÿæ‡‰èª¿æ•´
  # batch_size: 32 â†’ 64, learning_rate: 0.0005 â†’ 0.001
```

### æ¨¡å‹å®¹é‡èª¿æ•´

```yaml
model:
  architecture:
    emb_dim: 128          # å¯å¢åŠ åˆ° 192 æˆ– 256
    lstm_hidden_units: 128  # å¯å¢åŠ åˆ° 192 æˆ– 256
    n_layers: 4           # å¯å¢åŠ åˆ° 6 æˆ– 8
    n_heads: 4            # å¯å¢åŠ åˆ° 8
```

æ›´å¤šé…ç½®èªªæ˜è«‹åƒé–± [`../configs/README.md`](../configs/README.md)ã€‚

## ğŸ”„ è¨“ç·´ç­–ç•¥

### å¾é ­è¨“ç·´

```bash
./scripts/run_training.sh baseline
```

### å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´

ä¿®æ”¹é…ç½®æª”ï¼š

```yaml
training:
  resume_from_checkpoint: /workspace/experiments/logs/YYYYMMDD-HHMMSS-baseline/checkpoints/epoch_50.pth
```

### é·ç§»å­¸ç¿’

```yaml
model:
  pretrained_weights: /path/to/pretrained_model.pth
  freeze_encoder: false  # æ˜¯å¦å‡çµç·¨ç¢¼å™¨
```

## ğŸ“ˆ è¨“ç·´æŠ€å·§

### 1. æ‰¹æ¬¡å¤§å°å°‹æ‰¾

é€æ­¥å¢åŠ æ‰¹æ¬¡å¤§å°ç›´åˆ° OOMï¼Œç„¶å¾Œå›é€€ï¼š

```bash
# æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°
for bs in 16 32 64 128 256; do
    # ä¿®æ”¹ config batch_size=$bs
    # åŸ·è¡ŒçŸ­è¨“ç·´çœ‹æ˜¯å¦ OOM
done
```

### 2. å­¸ç¿’ç‡å°‹æ‰¾

ä½¿ç”¨å­¸ç¿’ç‡ finder æ‰¾åˆ°æœ€ä½³å­¸ç¿’ç‡ï¼š

```python
# åœ¨è¨“ç·´è…³æœ¬ä¸­æ·»åŠ 
from torch.optim.lr_scheduler import LRFinder
# ... å¯¦ä½œ LR finder
```

### 3. æ¢¯åº¦ç´¯ç©

ç•¶å—é™æ–¼è¨˜æ†¶é«”æ™‚ï¼š

```yaml
training:
  batch_size: 16
  gradient_accumulation:
    enabled: true
    steps: 8  # æœ‰æ•ˆæ‰¹æ¬¡ = 128
```

### 4. æ··åˆç²¾åº¦è¨“ç·´

å•Ÿç”¨ä»¥åŠ é€Ÿ 2-3xï¼š

```yaml
training:
  mixed_precision:
    enabled: true  # RTX 5090 æ”¯æ´
    opt_level: O1
```

### 5. è³‡æ–™å¢å¼·

è€ƒæ…®æ·»åŠ ï¼š
- æ™‚åŸŸå¢å¼·ï¼ˆtime stretching, pitch shiftingï¼‰
- é »åŸŸå¢å¼·ï¼ˆSpecAugmentï¼‰
- æ··åˆå¢å¼·ï¼ˆmixup, cutmixï¼‰

## â¹ï¸ åœæ­¢èˆ‡æ¢å¾©

### æ­£å¸¸åœæ­¢

è¨“ç·´æœƒåœ¨æ¯å€‹ epoch çµæŸæ™‚å„²å­˜æª¢æŸ¥é»ã€‚æŒ‰ `Ctrl+C` æ­£å¸¸åœæ­¢ã€‚

### å¾æª¢æŸ¥é»æ¢å¾©

```bash
# æ‰¾åˆ°æœ€æ–°æª¢æŸ¥é»
ls -lh experiments/logs/<experiment>/checkpoints/

# ä¿®æ”¹é…ç½®æª”æŒ‡å‘æª¢æŸ¥é»
# é‡æ–°å•Ÿå‹•è¨“ç·´
./scripts/run_training.sh continue-experiment
```

## ğŸ“ æœ€ä½³å¯¦è¸

1. **å°è¦æ¨¡æ¸¬è©¦**: å…ˆç”¨å°‘é‡è³‡æ–™èˆ‡å°‘æ•¸ epoch æ¸¬è©¦é…ç½®
2. **è¨˜éŒ„è©³ç´°**: å¡«å¯«å®Œæ•´çš„å¯¦é©—è¨˜éŒ„
3. **å®šæœŸå‚™ä»½**: é‡è¦æª¢æŸ¥é»è¤‡è£½åˆ°å®‰å…¨ä½ç½®
4. **ç›£æ§æŒ‡æ¨™**: é—œæ³¨ loss æ›²ç·šèˆ‡ GPU ä½¿ç”¨ç‡
5. **å°æ¯”å¯¦é©—**: ä¸€æ¬¡åªæ”¹è®Šä¸€å€‹è®Šæ•¸
6. **ç‰ˆæœ¬æ§åˆ¶**: é‡è¦çš„é…ç½®è®Šæ›´æäº¤åˆ° Git

## ğŸ“Š è©•ä¼°çµæœ

è¨“ç·´å®Œæˆå¾Œï¼š

```bash
# æŸ¥çœ‹è¨“ç·´æ—¥èªŒ
cat experiments/logs/<experiment>/training.log | grep "Validation"

# æŸ¥çœ‹æœ€ä½³æ¨¡å‹
ls -lh experiments/logs/<experiment>/checkpoints/best_model.pth

# åŸ·è¡Œè©•ä¼°ï¼ˆå¦‚æœ‰è©•ä¼°è…³æœ¬ï¼‰
# ./scripts/run_evaluation.sh <experiment>
```

## ğŸ”§ æ•…éšœæ’é™¤

### è¨“ç·´å¾ˆæ…¢

1. æª¢æŸ¥ GPU åˆ©ç”¨ç‡ï¼š`nvidia-smi`
2. å¢åŠ  `num_workers`
3. å•Ÿç”¨æ··åˆç²¾åº¦
4. æª¢æŸ¥è³‡æ–™è¼‰å…¥ç“¶é ¸

### Out of Memory

1. æ¸›å° batch_size
2. å•Ÿç”¨ gradient_checkpointing
3. ä½¿ç”¨ gradient accumulation
4. æ¸›å°æ¨¡å‹å¤§å°

### Loss ä¸ä¸‹é™

1. æª¢æŸ¥å­¸ç¿’ç‡ï¼ˆå¤ªå¤§æˆ–å¤ªå°ï¼‰
2. æª¢æŸ¥è³‡æ–™é è™•ç†
3. æª¢æŸ¥ loss function
4. å¢åŠ æ¨¡å‹å®¹é‡

### æ¢¯åº¦çˆ†ç‚¸

1. å•Ÿç”¨ gradient clipping
2. é™ä½å­¸ç¿’ç‡
3. æª¢æŸ¥è³‡æ–™æ­£è¦åŒ–

å®Œæ•´æ•…éšœæ’é™¤è«‹åƒé–± [TROUBLESHOOTING.md](TROUBLESHOOTING.md)ã€‚

## ğŸ“š å»¶ä¼¸é–±è®€

- [é…ç½®èªªæ˜](../configs/README.md)
- [å¯¦é©—è¨˜éŒ„æŒ‡å—](../experiments/README.md)
- [æ•…éšœæ’é™¤](TROUBLESHOOTING.md)

---

**ç¥è¨“ç·´é †åˆ©ï¼ğŸš€**
