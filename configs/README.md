# Configuration Files

æœ¬ç›®éŒ„åŒ…å«ä¸åŒå ´æ™¯çš„è¨“ç·´é…ç½®æª”æ¡ˆã€‚

## ğŸ“ æª”æ¡ˆèªªæ˜

### `training_rtx5090.yaml`
**RTX 5090 å„ªåŒ–é…ç½®ï¼ˆæ¨è–¦ï¼‰**

é‡å° NVIDIA RTX 5090 (32GB VRAM) å„ªåŒ–çš„é…ç½®ï¼š
- CUDA åŠ é€Ÿå•Ÿç”¨
- Batch size: 32 (å¯èª¿æ•´è‡³ 64-128)
- Mixed precision è¨“ç·´
- Gradient accumulation: 4 steps
- å¤š worker è³‡æ–™è¼‰å…¥

**é©ç”¨å ´æ™¯:**
- RTX 5090 å–® GPU è¨“ç·´
- åŸºæº–å¯¦é©—
- ç”Ÿç”¢ç’°å¢ƒè¨“ç·´

## ğŸ”§ é…ç½®åƒæ•¸èªªæ˜

### Hardware Section
```yaml
hardware:
  use_cuda: true          # ä½¿ç”¨ CUDA
  use_ddp: false          # å–® GPUï¼ˆå¤š GPU è¨­ç‚º trueï¼‰
  world_size: 1           # GPU æ•¸é‡
  device_ids: [0]         # CUDA è£ç½® ID
```

### Training Section - é—œéµåƒæ•¸

#### Batch Size
```yaml
batch_size: 32            # æ¯å€‹ GPU çš„æ‰¹æ¬¡å¤§å°
```

**èª¿æ•´å»ºè­°:**
- RTX 5090 (32GB): å¯ç”¨ 32-128
- ç›£æ§ GPU è¨˜æ†¶é«”ä½¿ç”¨
- é…åˆ gradient accumulation èª¿æ•´

#### Gradient Accumulation
```yaml
gradient_accumulation:
  enabled: true
  steps: 4                # æœ‰æ•ˆæ‰¹æ¬¡ = batch_size * steps
```

**èªªæ˜:**
- ç”¨å°æ‰¹æ¬¡æ¨¡æ“¬å¤§æ‰¹æ¬¡è¨“ç·´
- ç•¶å‰è¨­å®š: 32 * 4 = 128 æœ‰æ•ˆæ‰¹æ¬¡
- å¯æ¸›å°‘ steps æˆ–åœç”¨ï¼ˆå¦‚æœ batch_size å¤ å¤§ï¼‰

#### Mixed Precision
```yaml
mixed_precision:
  enabled: true           # è‡ªå‹•æ··åˆç²¾åº¦è¨“ç·´
  opt_level: O1           # FP16 + FP32 æ··åˆ
```

**å„ªé»:**
- 2-3x è¨“ç·´åŠ é€Ÿ
- æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ ~40%
- RTX 5090 tensor cores å„ªåŒ–

#### Learning Rate
```yaml
learning_rate: 0.0005     # åˆå§‹å­¸ç¿’ç‡
```

**èª¿æ•´åŸå‰‡:**
- æ‰¹æ¬¡å¤§å°å¢åŠ  â†’ å­¸ç¿’ç‡ç·šæ€§å¢åŠ 
- ä¾‹å¦‚: batch 32â†’64, LR 0.0005â†’0.001
- å»ºè­°ä½¿ç”¨ warmup

#### Data Loading
```yaml
num_workers: 4            # è³‡æ–™è¼‰å…¥ä¸¦è¡Œæ•¸
```

**å»ºè­°å€¼:**
- CPU æ ¸å¿ƒæ•¸çš„ 1/2 åˆ° 1/4
- ç›£æ§ CPU ä½¿ç”¨ç‡
- éå¤šæœƒå¢åŠ è¨˜æ†¶é«”ä½¿ç”¨

## ğŸ¯ å¯¦é©—é…ç½®ç¯„ä¾‹

### 1. å¿«é€Ÿå¯¦é©—ï¼ˆå°æ‰¹æ¬¡ï¼‰
```yaml
batch_size: 16
gradient_accumulation:
  enabled: false
mixed_precision:
  enabled: true
```

### 2. æœ€å¤§ååé‡ï¼ˆå¤§æ‰¹æ¬¡ï¼‰
```yaml
batch_size: 128
gradient_accumulation:
  enabled: false
mixed_precision:
  enabled: true
num_workers: 8
```

### 3. è¨˜æ†¶é«”å„ªåŒ–ï¼ˆé•·éŸ³è¨Šï¼‰
```yaml
batch_size: 8
max_audio_length: 32000  # 2 seconds
gradient_accumulation:
  enabled: true
  steps: 16              # æœ‰æ•ˆæ‰¹æ¬¡ = 128
use_gradient_checkpointing: true
```

### 4. å¤§æ¨¡å‹è¨“ç·´
```yaml
model:
  architecture:
    emb_dim: 256          # å¢åŠ æ¨¡å‹å®¹é‡
    lstm_hidden_units: 256
    n_layers: 6
    n_heads: 8

training:
  batch_size: 16          # æ¸›å°æ‰¹æ¬¡
  gradient_accumulation:
    steps: 8              # å¢åŠ ç´¯ç©
```

## ğŸ“Š æ€§èƒ½èª¿å„ªæŒ‡å—

### GPU è¨˜æ†¶é«”ç›£æ§
```bash
# è¨“ç·´æ™‚ç›£æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¨˜æ†¶é«”ä½¿ç”¨
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### æ‰¹æ¬¡å¤§å°å°‹æ‰¾
1. å¾å°æ‰¹æ¬¡é–‹å§‹ï¼ˆå¦‚ 16ï¼‰
2. å€å¢ç›´åˆ° OOM
3. å›é€€åˆ°æœ€å¤§å¯ç”¨æ‰¹æ¬¡çš„ 80%

### æ•ˆèƒ½åŸºæº–æ¸¬è©¦
```bash
# æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°çš„ååé‡
for bs in 16 32 64 128; do
    echo "Testing batch_size=$bs"
    # ä¿®æ”¹ config ä¸¦æ¸¬è©¦
done
```

## ğŸ”„ å¾åŸå§‹é…ç½®é·ç§»

å¦‚æœå¾ macOS MPS é…ç½®é·ç§»ï¼š

```diff
hardware:
-  use_cuda: false
+  use_cuda: true
-  device_ids: [mps]
+  device_ids: [0]

training:
-  batch_size: 16
+  batch_size: 32
-  mixed_precision:
-    enabled: false
+  mixed_precision:
+    enabled: true

misc:
-  num_workers: 0
+  num_workers: 4
```

## ğŸ“ å»ºç«‹æ–°é…ç½®

1. è¤‡è£½ `training_rtx5090.yaml`
2. ä¿®æ”¹å¯¦é©—åç¨±èˆ‡æè¿°
3. èª¿æ•´ç›®æ¨™åƒæ•¸
4. è¨˜éŒ„è®Šæ›´åˆ°å¯¦é©—æ—¥èªŒ

```bash
cp training_rtx5090.yaml training_my_experiment.yaml
vim training_my_experiment.yaml
```

## ğŸ” é…ç½®é©—è­‰

è¨“ç·´å‰é©—è­‰é…ç½®ï¼š

```bash
# åœ¨å®¹å™¨ä¸­åŸ·è¡Œ
python -c "import yaml; yaml.safe_load(open('configs/training_rtx5090.yaml'))"

# æˆ–ä½¿ç”¨è¨“ç·´è…³æœ¬çš„ test-only æ¨¡å¼
./scripts/run_training.sh my-exp configs/training_rtx5090.yaml --skip-memory-test
```

## ğŸ“š åƒè€ƒè³‡æº

- PyTorch Mixed Precision: https://pytorch.org/docs/stable/amp.html
- Gradient Accumulation: https://kozodoi.me/blog/20210219/gradient-accumulation
- Batch Size Tuning: https://wandb.ai/wandb_fc/tips/reports/How-to-Pick-the-Best-Batch-Size--VmlldzoyMTEzMTU

---

**éœ€è¦å¹«åŠ©ï¼Ÿ** æŸ¥çœ‹ `../docs/TROUBLESHOOTING.md`
